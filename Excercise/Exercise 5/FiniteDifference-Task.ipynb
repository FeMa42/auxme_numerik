{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivatives of a functions using Finite Differences - Differenzenquotient\n",
    "In the lecture, we learned different ways to calculate the derivative of a function:\n",
    "- finite differences\n",
    "    - forward difference\n",
    "    - central difference\n",
    "- automatic differentiation (AD)\n",
    "    - forward mode (future exercise)\n",
    "    - reverse mode (future exercise)\n",
    "\n",
    "> Note: A good explanation for the error in estimating derivatives and automatic differentiation can be found here: https://book.sciml.ai/notes/08-Forward-Mode_Automatic_Differentiation_(AD)_via_High_Dimensional_Algebras/ \n",
    "> Some parts of this exercise are based on these lecture notes.\n",
    "\n",
    "The derivative of a function $f(x)$ can be approximated by the finite differences:\n",
    "$$\n",
    "\\frac{d f}{d x} \\approx \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}\n",
    "$$\n",
    "This is called the forward difference. We can also use the central difference:\n",
    "$$\n",
    "\\frac{d f}{d x} \\approx \\frac{f(x + \\Delta x) - f(x - \\Delta x)}{2 \\Delta x}\n",
    "$$\n",
    "\n",
    "In the lecture we already discussed that the step size $\\Delta x$ has a big influence on the error of the approximation. We want to investigate this influence and visualize the error for different step sizes $\\Delta x$. What is the smallest number we can use for $\\Delta x$? What happens if we use such a small number? What happens if we use a larger number? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.activate(\"finitediff\")\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"FiniteDifferences\")\n",
    "Pkg.add(\"Printf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.activate(\"finitediff\")\n",
    "using FiniteDifferences\n",
    "using Printf\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first let's take a look at the forward and central finite difference methods. We let's start by defining a function we want to differentiate together with the differentiation so that we can estimate the error we are making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function and its derivative\n",
    "function f(x)\n",
    "    x^2 + sin(x)\n",
    "end\n",
    "\n",
    "function df(x)\n",
    "    2 * x + cos(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the point `x` at which we want to evaluate the derivative. An compare the forward finite difference method with the central finite difference method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point where we want to approximate the derivative\n",
    "x = 2.0\n",
    "\n",
    "# testing the finite difference methods for the function f(x) = x^2+sin(x)\n",
    "central_fdm(2, 1)(f, x) # 2 is the number of points and 1 is the order of the derivative\n",
    "forward_fdm(2, 1)(f, x)\n",
    "df(2)\n",
    "error_cforward = abs(forward_fdm(2, 1)(f, 2) - df(2))\n",
    "error_central = abs(central_fdm(2, 1)(f, 2) - df(2))\n",
    "@printf(\"Error for forward difference: %.14f\\n\", error_cforward)\n",
    "@printf(\"Error for central difference: %.14f\\n\", error_central)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Step Sizes\n",
    "\n",
    "Write a function which takes the function $f(x)$, a value for $x$, and a vector of step sizes $\\Delta x$ and returns the forward and central differences of $f(x)$ at $x$ for the different step sizes $\\Delta x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function FiniteDifferenceMethod(x::Real, our_eps::Vector, f::Function)\n",
    "    forward = # TODO: implement the forward finite difference method\n",
    "    central = # TODO: implement the central finite difference method\n",
    "    forward, central\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our implementation for different eps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps1 = 1e-2\n",
    "eps2 = 1e-10\n",
    "# FiniteDifferenceMethod(x, eps_range, f)\n",
    "forward1, central1 = FiniteDifferenceMethod(x, [eps1], f)\n",
    "forward2, central2 = FiniteDifferenceMethod(x, [eps2], f)\n",
    "\n",
    "@printf(\"With eps = %.2e:\\n\", eps1)\n",
    "@printf(\"Forward difference approximation: %.6f\\n\", forward1[1])\n",
    "@printf(\"Central difference approximation: %.6f\\n\", central1[1])\n",
    "@printf(\"Error forward difference approximation: %.10f\\n\", abs(forward1[1] - df(2)))\n",
    "@printf(\"Error central difference approximation: %.10f\\n\", abs(central1[1] - df(2)))\n",
    "@printf(\"\\nWith eps = %.2e:\\n\", eps2)\n",
    "@printf(\"Forward difference approximation: %.6f\\n\", forward2[1])\n",
    "@printf(\"Central difference approximation: %.6f\\n\", central2[1])\n",
    "@printf(\"Error forward difference approximation: %.10f\\n\", abs(forward2[1] - df(2)))\n",
    "@printf(\"Error central difference approximation: %.10f\\n\", abs(central2[1] - df(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Step Sizes based on the machine precision $\\epsilon$\n",
    "\n",
    "First lets check our machine precision $\\epsilon$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = eps(Float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this number tell us? Since floating point numbers are scaled there is a limit to the precision of the numbers we want to represent and perform operations with. This is however relative to the size of the number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show eps(1.0)\n",
    "@show eps(0.1)\n",
    "@show eps(0.01);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the lecture this can be a problem if we substract numbers that are close to each other. Let's check this again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϵ = 1e-10rand()\n",
    "@show ϵ\n",
    "@show (1+ϵ)\n",
    "@show ϵ2 = (1+ϵ) - 1\n",
    "@show (ϵ - ϵ2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding 1 to the small random number we lose the information of the last digits of the small number. When we substract 1 again we don't get the same number back. Hence, we have a loss of precision.\n",
    "\n",
    "Ok, so now we want to see how this influence our finite differences approach. We learned that $\\sqrt{\\epsilon}$ is a good choice for $\\Delta x$. In general we cannot expect a lower errer than approximately $\\sqrt{\\epsilon} \\approx 10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show sqrt(eps(Float64));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use $\\Delta x = 2.3*10^{-16}$ as the smallest number. For the largest number we use $\\Delta x = 0.1$. Let's plot the error for different step sizes $\\Delta x$.\n",
    "\n",
    "Plot the error of the the forward and central difference for the function $f(x) = x^2+sin(x)$ and for different step sizes $\\Delta x$ from $0.1$ to $10^{-16}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Plots\n",
    "using LinearAlgebra\n",
    "f(x) = x^2+sin(x)\n",
    "df(x) = 2*x+cos(x) # analytical derivative of f to estimate the error\n",
    "\n",
    "eps_length = 16\n",
    "eps_range = 10 .^ -range(1, stop=16, length=eps_length)\n",
    "eps_range[eps_length] = 2.3*10^-16 # set the last element to 2.3e-16\n",
    "x = 1.0\n",
    "real_dfx = df(x)\n",
    "df_forw_eps, df_cent_eps = FiniteDifferenceMethod(x, eps_range, f)\n",
    "# TODO: estimate derivative and error using FiniteDifferenceMethod\n",
    "error_forward = ...\n",
    "error_central = ...\n",
    "\n",
    "p1 = Plots.scatter(eps_range, error_forward, label=\"err(eps)\" , title=\"forward\", xlabel=\"log eps\", ylabel=\"log error\", xscale=:log10, yscale=:log10)\n",
    "p2= Plots.scatter(eps_range, error_central, label=\"err(eps)\" , title=\"central\", xlabel=\"log eps\", ylabel=\"log error\", xscale=:log10, yscale=:log10)\n",
    "Plots.plot(p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_range = 10 .^ -range(4, stop=6, length=1000)\n",
    "df_fd_eps, df_cd_eps = FiniteDifferenceMethod(x, eps_range, f)\n",
    "\n",
    "# TODO: estimate the error \n",
    "error = ... \n",
    "\n",
    "p3 = Plots.scatter(eps_range, abs.(df_cd_eps.-real_dfx), label=\"err(eps)\" , title=\"central\", xlabel=\"eps\", ylabel=\"error\", xscale=:log10, yscale=:log10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives of a function with multiple variables with finite differences\n",
    "We have learned that the partial derivative of a function $f(x,y)$ with respect to $x$ is estimated by keeping $y$ constant and vice versa. We can use this approach to estimate the partial derivative using finite differences. Since the gradient of a function $f(x,y)$ is defined as:\n",
    "$$\n",
    "\\nabla f = \\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "We can estimate the gradient with the partial derivative for each variable seperately.\n",
    "\n",
    "Let's try this for the function $f(x, y) = x^2 + x*y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x, y) = x^2 + x * y\n",
    "a, b = 1.0, 1.0 # We want to estimate the partial derivatives of f at (a,b)\n",
    "\n",
    "f_x(x) = f(x, b)\n",
    "f_y(y) = f(a, y)\n",
    "\n",
    "real_df_dx = 2 * a + b\n",
    "real_df_dy = a\n",
    "\n",
    "df_dx = # TODO: estimate partial derivative w.r.t. x using central difference\n",
    "df_dy = # TODO: estimate partial derivative w.r.t. y using central difference\n",
    "@show grad_f = [df_dx, df_dy]\n",
    "@show real_grad_f = [real_df_dx, real_df_dy];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have another equation $g(x,y) = x^2 + y^2$ we can calculate the total derivative of the system of equations $f(x,y)$ and $g(x,y)$ with respect to $x$ and $y$ which is the Jacobian matrix:\n",
    "$$\n",
    "\\mathbf{J} = \\begin{bmatrix}\n",
    "\\nabla f \\\\\n",
    " \\nabla g\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y} \\\\\n",
    "\\frac{\\partial g}{\\partial x} & \\frac{\\partial g}{\\partial y}\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(x, y) = x^2 + y^2\n",
    "\n",
    "g_x(x) = g(x, b)\n",
    "g_y(y) = g(a, y)\n",
    "\n",
    "real_dg_dx = 2 * a\n",
    "real_dg_dy = 2 * b\n",
    "\n",
    "dg_dx = # TODO: estimate partial derivative w.r.t. x using central difference\n",
    "dg_dy = # TODO: estimate partial derivative w.r.t. y using central difference\n",
    "@show grad_g = [dg_dx, dg_dy]\n",
    "@show real_grad_g = [real_dg_dx, real_dg_dy]\n",
    "@show jacobian_f = [df_dx df_dy; dg_dx dg_dy];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used an inefficient way to calculate the Jacobian. We needed to calculate the partial derivative of each function with respect to each variable. There are more efficient ways to calculate the Jacobian matrix like using colored Jacobians or automatic differentiation (next section).\n",
    "\n",
    "In practice you can use FiniteDifferences.jl to calculate the Jacobian matrix using finite differences. This package can be installed with `Pkg.add(\"FiniteDifferences\")`. \n",
    "Let's try this out for the function \n",
    "$$\n",
    "\\mathbf{f(x)} =  \\begin{bmatrix}\n",
    "x_1^2 + x_1*x_2 \\\\\n",
    "x_1^2 + x_2^2\n",
    "\\end{bmatrix} \n",
    "$$. \n",
    "We can use the function `jacobian` from the package FiniteDifferences.jl to calculate the Jacobian matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FiniteDifferences\n",
    "f(x) = [x[1]^2+x[1]*x[2]; \n",
    "        x[1]^2+x[2]^2]\n",
    "x = [1.0, 1.0]\n",
    "\n",
    "FiniteDifferences.jacobian(FiniteDifferences.central_fdm(2, 1), f, x)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FiniteDifferences\n",
    "f(x) = [x[1]^2+x[1]*x[2]; \n",
    "        x[1]^2+x[2]^2]\n",
    "x = [1.0, 1.0]\n",
    "\n",
    "FiniteDifferences.jacobian(FiniteDifferences.central_fdm(2, 1), f, x)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the central difference approximation is of order O(eps^2). It increases quadratically with eps and is smaller than the forward difference approximation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
