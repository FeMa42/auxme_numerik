{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks \n",
    "\n",
    "In this exercise we will learn how to use neural networks to solve regression problems. We will use a neural network to predict the currents of the actuators and the change in the joint angle of our Openmanipulator robot arm. In the lecture we learned that we need: \n",
    "- data to train the neural network\n",
    "- pre and post processing functions to prepare the data for the neural network and to interpret the output of the neural network\n",
    "- a neural network architecture\n",
    "- a loss function to calculate the error between the output of the neural network and the desired output\n",
    "- a forward function to calculate the output of the neural network\n",
    "- a Optimize function to update the weights of the neural network\n",
    "\n",
    "To build and train the neural network we will use the **flux.jl** package. This package provides a lot of functions to build and train neural networks. The documentation of the package can be found [here](https://fluxml.ai/Flux.jl/stable/).\n",
    "\n",
    "## Predicting currents in the Openmanipulator joints\n",
    "\n",
    "In this part we will use a neural network to predict the currents in our Openmanipulator robot arm. We will use real measured data of the currents and the joint angles of the first joint of our robot to train a neural network. First we will import the data and plot it to get a better understanding of the data. Secondly we will preprocess the data and train a neural network. Finally we will evaluate the performance of our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg                                          # Package manager\n",
    "Pkg.generate(joinpath(@__DIR__, \"NeuronaleNetze\"))  # activate package environment\n",
    "Pkg.activate(joinpath(@__DIR__, \"NeuronaleNetze\"))  # activate package environment\n",
    "Pkg.add(\"CSV\")                                      # add CSV\n",
    "Pkg.add(\"Plots\")                                    # add Plots\n",
    "Pkg.add(\"Statistics\")                               # add Statistics\n",
    "Pkg.add(\"ReverseDiff\")                              # add ReverseDiff\n",
    "Pkg.add(\"Flux\")                                     # add Machine Learning Library Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg                                          # Package manager\n",
    "Pkg.activate(joinpath(@__DIR__, \"NeuronaleNetze\"))  # activate package environment\n",
    "using CSV                                           # CSV Parser\n",
    "using Plots                                         # Graph Plotter\n",
    "using Statistics                                    # statisitical functions, like \"mean\"\n",
    "using Flux                                          # Machine Learning Library\n",
    "using ReverseDiff                                   # Reverse Mode Automatic Differentiation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "Using the Openmanipulator we collected data using the Dynamixel Wizard tool which enables us to control and measure each actuator of our Robot. An image of the measured test data is shown below. \n",
    "\n",
    "<img src=\"./OMP_Daten/pc-test.png\" width=\"800\">\n",
    "\n",
    "The data is stored in a CSV file. To load the CSV file, we can use the CSV module from Julia. For the input $x$ into our model we will read the columns for: \n",
    "- Present Position\n",
    "- Goal Position\n",
    "- Present Velocity\n",
    "\n",
    "We will make build the input matrix $x$ of these values.\n",
    "\n",
    "For the output $y$ we will read the columns for:\n",
    "- Present Current\n",
    "\n",
    "We will also make a matrix of these values. Since we are only predicting one value, we will only have one column. However, Flux expects a matrix, so we will make a matrix with one column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function load_csv_data(filename)\n",
    "    csv = CSV.File(filename)                        # load and parse csv\n",
    "    x_1 = csv.columns[3].column                     # mask column 3 (Present Position) - input 1\n",
    "    x_2 = csv.columns[4].column                     # mask column 4 (Goal Position) - input 2\n",
    "    x_3 = csv.columns[7].column                     # mask column 7 (Present Velocity) - input 3\n",
    "    y = csv.columns[5].column                       # mask column 5 (Present Current) - this is what we want to predict\n",
    "    x_train = hcat(x_1, x_2, x_3)'                  # create input matrix\n",
    "    y_train = y                                     # create output vector\n",
    "    y_train = reshape(y_train, 1, :)                # reshape output vector to a matrix (1 x n) - this is what Flux expects\n",
    "    x_train, y_train\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "x_train, y_train = load_csv_data(joinpath(@__DIR__, \"OMP_Daten/pc-train.csv\"))\n",
    "\n",
    "# load test data\n",
    "x_test, y_test = load_csv_data(joinpath(@__DIR__, \"OMP_Daten/pc-test.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To normalize the data we estimate the mean and standard deviation of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_x = mean(x_train)\n",
    "mean_y = mean(y_train)\n",
    "std_x = std(x_train)\n",
    "std_y = std(y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Functions for preprocessing and postprocessing the data\n",
    "\n",
    "1. Write a function to normalize the data using the mean and standard deviation.\n",
    "\n",
    "2. Write a function to denormalize the data using the mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pre procseeing (make it machine frindly)\n",
    "function dataPreProcess(x, y)\n",
    "    x = Float32.(x)\n",
    "    y = Float32.(y)\n",
    "    ...\n",
    "    x, y\n",
    "end\n",
    "\n",
    "# data post procseeing (make it human frindly)\n",
    "function dataPostProcess(x, y)\n",
    "    ...\n",
    "    x, y\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data using the function you wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "x_train, y_train = dataPreProcess(x_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the mean and std of the data to make sure we did it right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show mean(x_train)\n",
    "@show std(x_train)\n",
    "@show mean(y_train)\n",
    "@show std(y_train);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to normalize the test data. It is important to use the mean and std of the training data to normalize the test data since our neural network is trained on the normalized training data. Using a different mean and std for the test data would mean that we are using a different distribution for the test data than for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = dataPreProcess(x_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting with a Linear Model \n",
    "\n",
    "In the lecture we showed that perceptrons can be build out of simple linear functions and adding a nonlinear activation layer. So before we will implement a neural network we take a step back and implement a linear function which estimates the currents. \n",
    "\n",
    "The linear function is defined as:\n",
    "$$  \n",
    "\\tilde{y} = \\tilde{x}^T \\cdot \\tilde{w} + b\n",
    "$$\n",
    "where $x$ is the input vector and $w$ is the weight vector and $b$ is the bias. We changed the notation to avoid adding the bias as a additional weight: \n",
    "$$\n",
    "y = x^T \\cdot w = \\begin{bmatrix} 1 & x_1 & x_2 & \\dots & x_n \\end{bmatrix} \\cdot \\begin{bmatrix} w_b \\\\ w_1 \\\\ w_2 \\\\ \\dots \\\\ w_n \\end{bmatrix} = 1 \\cdot w_b + x_1 \\cdot w_1 + x_2 \\cdot w_2 + \\dots + x_n \\cdot w_n\n",
    "$$\n",
    "where $w$ is now the extended weight vector and $x$ is the input vector extended by a 1.\n",
    "\n",
    "First lets look at an example of how to calculate the output of the linear function. We will use the first 3 rows of the training data as an example. The weight vector $w$ is used to generate an output $y$ for each row of the matrix $X$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = x_train[:, 1:3] # first 3 training examples each having 3 values\n",
    "@show x_1 = vcat(ones(1, size(x_1, 2)), x_1)\n",
    "@show w_1 = [1, 1, 1, 1]\n",
    "@show x_1' * w_1;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Build a linear model\n",
    "\n",
    "1. Build a linear model using the definition we gave above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation\n",
    "function linear_forward(x, w)\n",
    "    x = ...\n",
    "\n",
    "    # Calculate the output of the neuron\n",
    "    y_hat = ...\n",
    "end\n",
    "\n",
    "# Define a function that takes in the weights and the data, and returns the loss\n",
    "function loss(w, x, y)\n",
    "    y_hat = ...\n",
    "    mse = sum((y_hat .- y) .^ 2) / size(y, 1)\n",
    "    return mse\n",
    "end\n",
    "\n",
    "# Define a function that computes the gradients of the loss with respect to the weights\n",
    "function grad(w, x, y)\n",
    "    return ReverseDiff.gradient(w -> loss(w, x, y), w)\n",
    "end\n",
    "\n",
    "# initialize model using random normal weights\n",
    "function train_linear_model(x, y, learning_rate, num_epochs)\n",
    "    w = randn(4, 1)\n",
    "    mse = loss(w, x_t, y_t)\n",
    "    println(\"Start: MSE = $mse\")\n",
    "    for i = 1:num_epochs\n",
    "        # Compute the gradients of the loss with respect to the weights\n",
    "        dw = grad(w, x, y)\n",
    "\n",
    "        # Update the weights using gradient descent\n",
    "        w -= learning_rate * dw\n",
    "\n",
    "        if i % 10 == 0\n",
    "            # Compute the loss and print it\n",
    "            mse = loss(w, x, y)\n",
    "            println(\"Iteration $i: MSE = $mse\")\n",
    "        end\n",
    "    end\n",
    "    return w\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start to train a linear model on the real dataset we will generate an artificial one where we know that we can actually learn it using a linear function. We will use the following to generate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "Random.seed!(1234)\n",
    "\n",
    "# Define the true weights\n",
    "w_true = randn(3, 1)\n",
    "\n",
    "# Generate the dataset\n",
    "n_samples = 1000\n",
    "x_t = randn(3, n_samples)\n",
    "y_t = x_t' * w_true\n",
    "\n",
    "# Add some noise to the targets\n",
    "y_t += 0.1 * randn(n_samples, 1);\n",
    "\n",
    "w = train_linear_model(x_t, y_t, 0.01, 200)\n",
    "\n",
    "@show w_true\n",
    "@show w;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Train the linear model on the current dataset \n",
    "Now lots try to train the model on the real data. We already extracted the data from the CSV file and normalized it. So we can just start the training process by passing `x_train`and `y_train` to the `train_linear_model` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_linear_model(x_train, y_train, 0.01, 200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this does not seem to work. Can you think of a reason why? \n",
    "\n",
    "### Task: Build a linear model using Flux\n",
    "We can also build and train a linear model in a few lines of code using FLux. Build a linear model using Flux and train it on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "# building a linear  model using Flux\n",
    "linear_model = ...\n",
    "                    \n",
    "\n",
    "# Define the loss function\n",
    "loss(x, y) = sum((linear_model(x) .- y) .^ 2) / size(y, 1)\n",
    "\n",
    "# Define the optimizer\n",
    "opt = Descent(0.01) # you can also try it with the ADAM optimizer \n",
    "\n",
    "# Train the model - check the train function and what we pass to it\n",
    "# Note that the train function is called with train! which means that it will modify the model parameters passed to it\n",
    "numIter = 200\n",
    "for i in 1:numIter\n",
    "    Flux.train!(loss, Flux.params(linear_model), [(x_train, y_train)], opt, cb=() -> println(loss(x_train, y_train)))\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model with nonlinear activation function using Flux\n",
    "Now we extend the linear model by adding a nonlinear activation function. We will use the sigmoid function as activation function using `Flux.sigmoid`. Our function is then defined as:\n",
    "$$\n",
    "y = h(x^T \\cdot w) = h(\\begin{bmatrix} 1 & x_1 & x_2 & \\dots & x_n \\end{bmatrix} \\cdot \\begin{bmatrix} w_b \\\\ w_1 \\\\ w_2 \\\\ \\dots \\\\ w_n \\end{bmatrix}) = h(1 \\cdot w_b + x_1 \\cdot w_1 + x_2 \\cdot w_2 + \\dots + x_n \\cdot w_n)\n",
    "$$\n",
    "where $h$ is the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build linear model with nonlinear sigmoid activation function\n",
    "non_linear_model = ...\n",
    "\n",
    "# Define the loss function\n",
    "loss(x, y) = sum((non_linear_model(x) .- y) .^ 2) / size(y, 1)\n",
    "\n",
    "# Define the optimizer\n",
    "opt = Descent(0.01) # you can also try it with the ADAM optimizer \n",
    "\n",
    "# Train the model\n",
    "numIter = 200\n",
    "for i in 1:numIter\n",
    "    Flux.train!(loss, Flux.params(non_linear_model), [(x_train, y_train)], opt, cb=() -> println(loss(x_train, y_train)))\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network \n",
    "\n",
    "Now we have seen that we cannot learn the current of the actuators using a linear model. We also adapted the linear model with a nonlinear activation function. Now we will use several layers of these linear functions with nonlinear activation function to build a feed forward neural network. We will use a fully connected feed forward neural network with 1-2 hidden layers and a linear output layer. To build such a fully connected feed forward neural network we can use the `chain` function from Flux. The `chain` function takes a list of layers as input and builds a neural network with these layers. In this case we will use `Dense` layers which are fully connected layers. The `Dense` layer takes the number of input neurons, the number of output neurons and the activation function as input."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Build the neural network\n",
    "\n",
    "Build a neural network with 1-2 hidden layer. The input layer should have 3 neurons and the output layer should have 1 neuron to fit our data. You can also test different activation functions and different numbers of neurons in the hidden layer. \n",
    "\n",
    "You can check the different activation function in flux here: https://fluxml.ai/Flux.jl/stable/models/activation/ \n",
    "\n",
    "To optimize the neural network we will use the ADAM optimizer and set its learning rate to $0.001$. You can check the different optimizers in flux here: https://fluxml.ai/Flux.jl/stable/training/optimisers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our Neural Network model\n",
    "function build_model()\n",
    "    opt = ...\n",
    "    input_size = size(x_train)[1]\n",
    "    output_size = size(y_train)[1]\n",
    "    model = Chain(Dense(input_size, ...),\n",
    "        ...,\n",
    "        Dense(..., output_size, identity))\n",
    "    model, opt\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: define the forward function\n",
    "\n",
    "We also define a forward function to calculate the output of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function forward(x)\n",
    "    y_hat = model(x)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: define the loss function\n",
    "\n",
    "We also need a loss function to optimize. Since we are doing regression, we will use the mean squared error loss function. You can check the different loss functions in flux here: https://fluxml.ai/Flux.jl/stable/models/losses/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean square error as the loss function for optimization\n",
    "function loss(y_hat, y)\n",
    "    sum((y_hat .- y) .^ 2)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Train the neural network\n",
    "\n",
    "Now we will put everything together and train the neural network. We will define a function which takes the neural network, the optimizer, and the number of epochs as input. The function should return the loss based on the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "function train_model(model, opt, numIter)\n",
    "    trainLoss = zeros(numIter)\n",
    "    testLoss = zeros(numIter)\n",
    "    opt_state = Flux.setup(opt, model)\n",
    "    for i in 1:numIter\n",
    "        Flux.train!(model, [(x_train, y_train)], opt_state) do m, x, y\n",
    "            loss(m(x), y)\n",
    "        end\n",
    "        trainLoss[i] = loss(forward(x_train), y_train) / length(y_train)\n",
    "        testLoss[i] = loss(forward(x_test), y_test) / length(y_test)\n",
    "    end\n",
    "    trainLoss, testLoss\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the `build_model()` and `train_model()` functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = build_model() # build model\n",
    "trainLoss, testLoss = train_model(model, opt, 1000) # train model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the train and test loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the train loss \n",
    "plot(trainLoss, label=\"Train Loss\", xlabel=\"Iterations\", ylabel=\"loss\")\n",
    "\n",
    "# plot the test loss\n",
    "plot!(testLoss, label=\"Test Loss\", xlabel=\"Iterations\", ylabel=\"loss\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the predicted currents and the actual currents based on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results for the training data\n",
    "y_hat = forward(x_train)\n",
    "plot(y_train[1, :], label=\"Ground Truth (train)\", ylabel=\"Current\")\n",
    "plot!(y_hat[1, :], label=\"Prediction (train)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the final model on the test data\n",
    "y_hat = forward(x_test)\n",
    "error = sum((y_hat .- y_test) .^ 2) / length(y_test)\n",
    "println(\"Test Error: \", error)\n",
    "\n",
    "# post process the data (make it human readable)\n",
    "x_test, y_test = dataPostProcess(x_test, y_test)\n",
    "_, y_hat = dataPostProcess(x_test, y_hat)\n",
    "\n",
    "# plot the results\n",
    "plot(y_test[1, :], label=\"Ground Truth (test)\", ylabel=\"Current\")\n",
    "plot!(y_hat[1, :], label=\"Prediction (test)\")\n",
    "\n",
    "# error in human readable form\n",
    "error = y_hat .- y_test\n",
    "\n",
    "# plot the error\n",
    "plot!(error[1, :], label=\"Error\", ylabel=\"Error\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Could we replace a simulation with a neural network?\n",
    "\n",
    "In this part we will investigate how well a neural network can predict the change of the joint angle of the first joint of the Openmanipulator-X based on the joint angle, the goal joint angle and the current in the actuator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function load_csv_data(filename)\n",
    "    csv = CSV.File(filename)                        # load and parse csv\n",
    "    x_1 = csv.columns[3].column[1:end-1]            # mask column 3 (Present Position) \n",
    "    x_2 = csv.columns[4].column[1:end-1]            # mask column 4 (Goal Position) \n",
    "    x_3 = csv.columns[5].column[1:end-1]            # mask column 5 (Present Current) \n",
    "    # the prediction will be the delta position of the joint (Present Position at time t+1 - Present Position at time t)\n",
    "    y = csv.columns[3].column[2:end] - csv.columns[3].column[1:end-1] # mask column 3 (delta Position) - this is what we want to predict\n",
    "\n",
    "    x_train = hcat(x_1, x_2, x_3)'                  # create input matrix\n",
    "    y_train = y                                     # create output vector\n",
    "    y_train = reshape(y_train, 1, :)                # reshape output vector to a matrix (1 x n) - this is what Flux expects\n",
    "    x_train, y_train\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "x_train, y_train = load_csv_data(joinpath(@__DIR__, \"OMP_Daten/pc-train.csv\"))\n",
    "# load test data\n",
    "x_test, y_test = load_csv_data(joinpath(@__DIR__, \"OMP_Daten/pc-test.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the training data to get a better understanding of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data and set the heading \n",
    "p1 = plot(x_train[1, :], label=\"P-Pos\", ylabel=\"x_1\")\n",
    "p2 = plot(x_train[2, :], label=\"G-Pos\", ylabel=\"x_2\")\n",
    "p3 = plot(x_train[3, :], label=\"P-I\", ylabel=\"x_3\")\n",
    "p4 = plot(y_train[1, :], label=\"âˆ‚-Pos\", ylabel=\"y\")\n",
    "\n",
    "plot(p1, p2, p3, p4, layout=(4, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean and standard deviation of the training data\n",
    "mean_x = mean(x_train)\n",
    "mean_y = mean(y_train)\n",
    "std_x = std(x_train)\n",
    "std_y = std(y_train)\n",
    "# Data Preprocessing\n",
    "x_train, y_train = dataPreProcess(x_train, y_train)\n",
    "x_test, y_test = dataPreProcess(x_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the model and train it. This time we will train it for $20000$ epochs. The rest of the parameters stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = build_model() # build model\n",
    "trainLoss, testLoss = train_model(model, opt, 20000) # train model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the train and test loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the train loss\n",
    "fig = plot(1:numIter, trainLoss, label=\"Train Loss\", xlabel=\"Iterations\", ylabel=\"loss\")\n",
    "# plot the test loss\n",
    "plot!(1:numIter, testLoss, label=\"Test Loss\", xlabel=\"Iterations\", ylabel=\"loss\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how well the model predicts the joint angle change based on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results for the training data\n",
    "y_hat = forward(x_train)\n",
    "plot(y_train[1, :], label=\"Ground Truth (train)\", ylabel=\"Position\")\n",
    "plot!(y_hat[1, :], label=\"Prediction (train)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the final model on the test data\n",
    "y_hat = forward(x_test)\n",
    "error = sum((y_hat .- y_test) .^ 2) / length(y_test)\n",
    "println(\"Test Error: \", error)\n",
    "\n",
    "# post process the data (make it human readable)\n",
    "x_test, y_test = dataPostProcess(x_test, y_test)\n",
    "_, y_hat = dataPostProcess(x_test, y_hat)\n",
    "\n",
    "# plot the results\n",
    "plot(y_test[1, :], label=\"Ground Truth (test)\", ylabel=\"Position\")\n",
    "plot!(y_hat[1, :], label=\"Prediction (test)\")\n",
    "\n",
    "# error in human readable form\n",
    "error = y_hat .- y_test\n",
    "\n",
    "# plot the error\n",
    "plot!(error[1, :], label=\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the final model on the test data\n",
    "y_hat = forward(x_test)\n",
    "error = sum((y_hat .- y_test) .^ 2) / length(y_test)\n",
    "println(\"Test Error: \", error)\n",
    "\n",
    "# post process the data (make it human readable)\n",
    "x_test, y_test = dataPostProcess(x_test, y_test)\n",
    "_, y_hat = dataPostProcess(x_test, y_hat)\n",
    "\n",
    "# plot the results\n",
    "plot(y_test[1, :], label=\"Ground Truth (test)\", ylabel=\"Position\")\n",
    "plot!(y_hat[1, :], label=\"Prediction (test)\")\n",
    "\n",
    "# error in human readable form\n",
    "error = y_hat .- y_test\n",
    "\n",
    "# plot the error\n",
    "plot!(error[1, :], label=\"Error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.0",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
