{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivatives of a function with multiple variables\n",
    "\n",
    "In the last exercise we: \n",
    "- implemented the forward kinematic for the Openmanipulator-X\n",
    "- and implemented a way to solve the inverse kinematic with NLSolve.jl\n",
    "\n",
    "What did we actually do when we used the NLSolve.jl package? We used an implementation of a root finding method to find the solution to our inverse kinematic problem. We learned in the lecture how to find the root of a set of functions with multiple variables. For example we learned that we can find the root of a set of functions with the Newton method. Can we actually implement the Newton method for multiple variables ourself? \n",
    "\n",
    "Well we need to know how to calculate the derivative of a function with multiple variables. In the next exercise we will take a closer look into the Newton Method for multiple variables. For now we will just look at the algorithm of the Newton method for multiple variables. Similar to the one dimensional case we calculate the next $\\mathbf{x}$ which is closer to the root with the following calculation rule:\n",
    "\n",
    "$$\n",
    "\\begin{split}\\begin{split}\n",
    "  \\mathbf{x}_{n+1} &= \\mathbf{x}_n - \\mathbf{J}(\\mathbf{x}_n)^{-1}\\mathbf{f}(\\mathbf{x}_n).\n",
    "\\end{split}\\end{split}\n",
    "$$\n",
    "The calculation rule for the next $\\mathbf{x}$ which is closer to the root is thus iteratively called over and over again. The calculation rule according to the Newton method is therefore:\n",
    "$$\n",
    "\\mathbf{x}_0 = \\text{start value}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{x}_{1} = \\mathbf{x}_0 - \\mathbf{J}(\\mathbf{x}_0)^{-1}\\mathbf{f}(\\mathbf{x}_0)\n",
    "$$\n",
    "$$\n",
    "\\mathbf{x}_{2} = \\mathbf{x}_{1} - \\mathbf{J}(\\mathbf{x}_{1})^{-1}\\mathbf{f}(\\mathbf{x}_{1})\n",
    "$$\n",
    "...\n",
    "$$\n",
    "\\mathbf{x}_{n+1} = \\mathbf{x}_n - \\mathbf{J}(\\mathbf{x}_n)^{-1}\\mathbf{f}(\\mathbf{x}_n)\n",
    "$$\n",
    "We do this until we have reached the desired accuracy, i.e., the desired distance of $\\mathbf{f}(\\mathbf{x}_{n+1})$ to $\\mathbf{0}$.\n",
    "\n",
    "In the calculation rule we see that we need to calculate the function $\\mathbf{f}(\\mathbf{x})$, the derivative of the function (the Jacobian $\\mathbf{J}(\\mathbf{x})$) and the inverse of the Jacobian matrix $\\mathbf{J}(\\mathbf{x})^{-1}$.\n",
    "\n",
    "**We are still missing some Important Parts...**\n",
    "\n",
    "So let's start with estimating the derivative of a function with multiple variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivatives of a functions with multiple variables\n",
    "\n",
    "## Partial Derivatives\n",
    "In the lecture we have already seen the partial derivative of a function with multiple variables. We can calculate the partial derivative of a function $f(x,y)$ with respect to $x$ by treating $y$ as a constant:\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} \n",
    "$$\n",
    "We can interpret this as how much $f(x,y)$ changes when $x$ changes. We can do this for all variables of $f$. Hence, we can also calculate the partial derivative of the function $f(x,y)$ with respect to $y$ by treating $x$ as a constant. \n",
    "\n",
    "For our example from above $f(x,y) = 0.5*sin(x) + cos(y)$ we would get:\n",
    "$$\n",
    "\\frac{\\partial f(x,y)}{\\partial x} = 0.5*cos(x)\n",
    "$$\n",
    "for the partial derivative with respect to $x$ and \n",
    "$$\n",
    "\\frac{\\partial f(x,y)}{\\partial y} = -sin(y)\n",
    "$$\n",
    "for the partial derivative with respect to $y$.\n",
    "\n",
    "As an example, let's take a look again at the plot of the lecture were we plotted the partial derivative of $f(x,y)$ with respect to $x$ and $y$ seperately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "include(\"./PartialDerivativesPlot.jl\")\n",
    "using .PartialDerivativesPlot\n",
    "using GLMakie\n",
    "\n",
    "function parabola(x, y)\n",
    "    return @. x^2 + y^2\n",
    "end\n",
    "\n",
    "angle = LinRange(0, 2*pi, 64)\n",
    "radius = LinRange(0, 1, 16)\n",
    "\n",
    "x = radius .* cos.(angle')\n",
    "y = radius .* sin.(angle')\n",
    "z = parabola(x, y)\n",
    "\n",
    "tangentPoint=[0.0, 0.5]\n",
    "\n",
    "with_theme(theme_dark()) do\n",
    "    fig = Figure(resolution = (1000, 950), fontsize = 22)\n",
    "    ax = LScene(fig[1,1], show_axis = true)\n",
    "\n",
    "    sm = surface!(ax, x, y, z; colormap = :viridis, colorrange = (minimum(z), maximum(z)),\n",
    "        transparency = false)\n",
    "\n",
    "    # partial derivative w.r.t. x\n",
    "    PartialDerivativesPlot.directionalbox!(ax, parabola, tangentPoint, :x, tangentColor=GLMakie.Colors.HSL(0, 1, 0.4))\n",
    "\n",
    "    # partial derivative w.r.t. y\n",
    "    PartialDerivativesPlot.directionalbox!(ax, parabola, tangentPoint, :y, tangentColor=GLMakie.Colors.HSL(125, 1, 0.4))\n",
    "\n",
    "    display(fig)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Derivatives\n",
    "\n",
    "The total derivative of a function $f(x,y)$ with respect to $x$ and $y$ is defined as:\n",
    "$$\n",
    "d f = \\frac{\\partial f}{\\partial x} {d x} + \\frac{\\partial f}{\\partial y} {d y} =  f_x {d x} + f_y {d y} \n",
    "$$\n",
    "\n",
    "Remember, the total derivative gives us the relationship of changes in the function $f(x,y)$ when $x$ and $y$ change. Hence $d x$ and $d y$ are not variables we can calculate with: $d x \\neq \\Delta x$. \n",
    "We can make an approximation: \n",
    "$$\n",
    "\\Delta x \\approx \\frac{\\partial f}{\\partial x} {\\Delta x} + \\frac{\\partial f}{\\partial y} {\\Delta y}\n",
    "$$\n",
    "\n",
    "## Chain rule\n",
    "If we want to calculate the derivative of a function $f(x(t),y(t))$ with respect another variable $t$, where $x(t)$ and $y(t)$ depend on $t$ we can use the chain rule:\n",
    "$$\n",
    "\\frac{d f}{d t} = \\frac{\\partial f}{\\partial x} \\frac{d x}{d t} + \\frac{\\partial f}{\\partial y} \\frac{d y}{d t}\n",
    "$$\n",
    "This can also be written in a more general way:\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial t} = \\frac{\\partial f}{\\partial x} \\frac{\\partial x}{\\partial t} + \\frac{\\partial f}{\\partial y} \\frac{\\partial y}{\\partial t}\n",
    "$$\n",
    "This form is also valid for functions with more indepedant variables. \n",
    "\n",
    "## Gradient\n",
    "The gradient of a function $f(x,y)$ is defined as:\n",
    "$$\n",
    "\\nabla f = \\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and we can write the total derivative as:\n",
    "$$\n",
    "d f = \\nabla f \\cdot \\begin{bmatrix}\n",
    "d x \\\\\n",
    "d y\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Jacobian Matrix\n",
    "If we want to calculate the total derivative of a system of equations, we can define a function as $\\mathbf{f}= \\begin{bmatrix} f_1 \\\\ f_2 \\end{bmatrix}$ and the total derivative of this function is defined as:\n",
    "$$\n",
    "d \\mathbf{f} = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y} \\\\\n",
    "\\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y}\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "d x \\\\\n",
    "d y\n",
    "\\end{bmatrix}\n",
    "= \\mathbf{J} \\cdot \\begin{bmatrix}\n",
    "d x \\\\\n",
    "d y\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "The matrix $\\mathbf{J}$ is called the Jacobian matrix of the function $\\mathbf{f}$. The Jacobian matrix is a matrix of all partial derivatives of the function $\\mathbf{f}$ with respect to all variables. The columns of the Jacobian matrix are the gradients of the function $\\mathbf{f}$ with respect to the variables: \n",
    "$$\n",
    "\\mathbf{J} = \\begin{bmatrix}\n",
    "\\nabla f_1 \\\\\n",
    " \\nabla f_2\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y} \\\\\n",
    "\\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y}\n",
    "\\end{bmatrix} \n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivatives of a functions \n",
    "In the lecture, we learned different ways to calculate the derivative of a function:\n",
    "- finite differences\n",
    "    - forward difference\n",
    "    - central difference\n",
    "- automatic differentiation (AD)\n",
    "    - forward mode\n",
    "    - reverse mode\n",
    "\n",
    "A good explanation for the error in estimating derivatives and automatic differentiation can be found here: https://book.sciml.ai/notes/08-Forward-Mode_Automatic_Differentiation_(AD)_via_High_Dimensional_Algebras/\n",
    "Some parts of this exercise are based on these lecture notes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation (AD)\n",
    "Automatic Differentiation (AD) is a set of techniques to numerically evaluate the derivative of a function. AD is generally more accurate than the finite differences based numerical differentiation we used above. In the lecture we have seen two different methods of AD: the forward mode and the reverse mode (Backpropagation).\n",
    "\n",
    "### Forward Mode\n",
    "You have seen in the lecture that the forward mode AD makes use of dual numbers to calculate the derivative of a function. For each value $x$ we create a dual number $x + \\epsilon x'$ where $\\epsilon$ is a small number and $x'$ is the derivative of $x$. We can use the dual number to calculate the derivative of a function $f(x)$:\n",
    "$$\n",
    "f(x + \\epsilon) = f(x) + \\epsilon f'(x) + \\mathcal{O}(\\epsilon^2)\n",
    "$$\n",
    "As discussed in the lecture we defined $\\epsilon^2 = 0$ and got:\n",
    "$$\n",
    "f(x + \\epsilon) = f(x) + \\epsilon f'(x)\n",
    "$$\n",
    "\n",
    "To use this in practice we have to implement $f(x)$ to support our dual numbers for each function we want to calculate the derivative. Hence, we have to implement what happens with the derivative $f'(x)$ for each operation we want to support. For example, for the addition of two dual numbers $x + \\epsilon x'$ and $y + \\epsilon y'$ we get:\n",
    "$$\n",
    "(x + \\epsilon x') + (y + \\epsilon y') = (x + y) + \\epsilon (x' + y')\n",
    "$$ \n",
    "We can see that the derivative of the addition is the addition of the derivatives. We can implement this in Julia by overloading the addition operator `+` for dual numbers. We can do this similarly for the other operations.\n",
    "\n",
    "We can use the package `ForwardDiff.jl` to do this for us. Let's try this out for the function $f(x) = x^2 + sin(x)$. We can use the function `ForwardDiff.derivative` to calculate the derivative of $f(x)$ at $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ForwardDiff\n",
    "f(x) = x^2+sin(x)\n",
    "df(x) = 2*x+cos(x) # analytical derivative of f to estimate the error\n",
    "\n",
    "@show # TODO: estimate derivative and error using ForwardDiff.derivative\n",
    "@show df(1.0);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the gradient of a function $f(x,y)$ with respect to $x$ and $y$ using the function `ForwardDiff.gradient`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = x[1]^2+x[1]*x[2]\n",
    "\n",
    "@show # TODO: estimate the gradient of f at x, y using ForwardDiff.gradient"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the Jacobian matrix using the function `jacobian` from `ForwardDiff`. This function takes a function as an argument and returns the Jacobian matrix of the function. Let's try this out for the function \n",
    "$$\n",
    "\\mathbf{f(x)} =  \\begin{bmatrix}\n",
    "x_1^2 + x_1*x_2 \\\\\n",
    "x_1^2 + x_2^2\n",
    "\\end{bmatrix} \n",
    "$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = [x[1]^2+x[1]*x[2]; \n",
    "        x[1]^2+x[2]^2]\n",
    "x = [1.0, 1.0]\n",
    "\n",
    "@show # TODO: estimate the Jacobian of f at x using ForwardDiff.jacobian"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse Mode\n",
    "When we want to compute the gradient of a function $f(x)$ with respect to $x$ we have seen that we can use the forward mode AD. This is a good choice if we have a function $f(x)$ with a small number of variables. If we have a function $f(x)$ with a large number of variables the forward mode AD is not efficient. In this case we can use the reverse mode AD (backpropagation, adjoint technique). In the lecture we have seen that the reverse mode AD uses the chain rule to calculate the gradient of a function $f(x(z), y(z))$ with respect to $z$. We can write the chain rule as:\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial z} = \\frac{\\partial f}{\\partial x} \\frac{\\partial x}{\\partial z} + \\frac{\\partial f}{\\partial y} \\frac{\\partial y}{\\partial z}\n",
    "$$\n",
    "where $x, y$ are intermediate variables. We can use this to calculate the gradient of a function $f(x(z), y(z))$ with respect to $z$ by calculating the partial derivative of $f(x(z), y(z))$ with respect to each intermediate variable $x$ or $y$ and multiplying it with the partial derivative of $x$ or $y$ with respect to $z$. We can see that we have to calculate the partial derivative of $f$ with respect to each intermediate variable $x,y $ and the partial derivative of $x,y $ with respect to $z$. A popular example for the use of reverse mode AD is in deep learning to calculate the gradient of the loss function with respect to the weights of the neural network. In this case we have a large number of weights and often a scalar loss function. Hence, the reverse mode AD is a good choice. Now we want to take a look at the reverse mode AD in practice. We will use the package `ReverseDiff.jl` to do this. Let's try this out for the function $f(x_1, x_2) = x_1^2 + x_1*x_2$. \n",
    "\n",
    "If you are interested in the details of reverse mode AD you can have a look at the lecture slides again or find a good explanation here: https://book.sciml.ai/notes/10-Basic_Parameter_Estimation-Reverse-Mode_AD-and_Inverse_Problems/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"ReverseDiff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ReverseDiff\n",
    "\n",
    "f(x) = x[1]^2+x[1]*x[2]\n",
    "@show # TODO: estimate the gradient of f at x, y using ReverseDiff.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = [x[1]^2+x[1]*x[2]; \n",
    "        x[1]^2+x[2]^2]\n",
    "@show # TODO: estimate the Jacobian of f at x using ReverseDiff.jacobian"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see for the basic usage `ReverseDiff` and `ForwardDiff` are very similar. Which one you should use depends on the function you want to differentiate. It is very well summarized in https://book.sciml.ai: Forward mode computes columns of a Jacobian, while reverse mode computes gradients (rows of a Jacobian). Therefore, the relative efficiency of the two approaches is based on the size of the Jacobian. \n",
    "For $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ with $n$ variables and $m$ functions the Jacobian is a $m \\times n$ matrix. So computing each row will be much faster if $m$ is much smaller than $n$. Or in other words if the number of variables is larger than the number of functions, then reverse mode is more efficient. If the number of functions is larger than the number of variables, then forward mode is more efficient. If $m$ and $n$ are relatively equal forward mode is often more efficient since reverse mode has a high overhead compared to forward mode. If you are not sure which approach to use you can try both and benchmark the results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation of the end effector velocity based on the joint velocities\n",
    "\n",
    "We can measure the joint velocities of our Openmanipulator. However, we are interested in the speed of the end effector. We have seen in the lecture that we can calculate the end effector velocity based on the joint velocities using the Jacobian matrix. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Jacobian matrix of the Openmanipulator-X kinematics\n",
    "To estimate the velocity of the end effector we need to know the forward kinematics of the Openmanipulator-X and estimate the derivative of the forward kinematics. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.activate(\"../Exercise 3/OpenMEnv\") \n",
    "using Revise\n",
    "include(\"../OpenManipulatorLib/OpenManipulatorKinematics.jl\")\n",
    "using .OpenManipulatorKineamtics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the jacobian matrix of the forward kinematics for our robot arm using automatic differentiation. We can use the function `ForwardDiff.jacobian` to calculate the Jacobian matrix of the forward kinematics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff\n",
    "\n",
    "function openmanipulator_jacobian(q)\n",
    "    # Define a function that returns the position of the end effector\n",
    "    f(q) = # TODO: calculate the position of the end effector using OpenManipulatorKinematics.forward_kinematics \n",
    "\n",
    "    # Calculate the Jacobian matrix using ForwardDiff.jacobian\n",
    "    J = # TODO: calculate the Jacobian matrix using ForwardDiff.jacobian\n",
    "\n",
    "    return J\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the function `ReverseDiff.jacobian` to calculate the Jacobian matrix of the forward kinematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ReverseDiff\n",
    "\n",
    "function openmanipulator_jacobian_rev_AD(q)\n",
    "    # Define a function that returns the position of the end effector\n",
    "    f(q) = # TODO: calculate the position of the end effector using OpenManipulatorKinematics.forward_kinematics\n",
    "\n",
    "    # Calculate the Jacobian matrix using ReverseDiff.jacobian\n",
    "    J = # TODO: calculate the Jacobian matrix using ReverseDiff.jacobian\n",
    "\n",
    "    return J\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now estimate the jacobian matrix using finite differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FiniteDifferences\n",
    "\n",
    "function openmanipulator_jacobian_finiteDiff(q)\n",
    "    # Define a function that returns the position of the end effector\n",
    "    f(q) = # TODO: calculate the position of the end effector using OpenManipulatorKinematics.forward_kinematics\n",
    "    \n",
    "    # Calculate the Jacobian matrix FiniteDifferences \n",
    "    J = # TODO: calculate the Jacobian matrix using FiniteDifferences.jacobian\n",
    "\n",
    "    return J\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets compare the time and results of the methods. What do you observe?\n",
    "\n",
    "> Call all cells a second time to see if the results change? Why do you think the results change? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = [0.1, 0.0, 0.0, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time openmanipulator_jacobian(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time openmanipulator_jacobian_rev_AD(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time openmanipulator_jacobian_finiteDiff(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare again where we run the calculation several times for each approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function jacobian_random_benchmark(jacobian_function)\n",
    "    for i in 1:10000\n",
    "        q_init = rand(4)\n",
    "        jacobian_function(q_init)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time jacobian_random_benchmark(openmanipulator_jacobian)\n",
    "@time jacobian_random_benchmark(openmanipulator_jacobian_rev_AD)\n",
    "@time jacobian_random_benchmark(openmanipulator_jacobian_finiteDiff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also estimate the performance of the different methods by using the `BenchmarkTools.jl` package. We can use the `@benchmark` macro to measure the time and memory efficiency of the calculation. Let's try this out for the different methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkg.add(\"BenchmarkTools\")\n",
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark openmanipulator_jacobian(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark openmanipulator_jacobian_rev_AD(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark openmanipulator_jacobian_finiteDiff(q)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating of the end effector velocity using the Jacobian matrix\n",
    "\n",
    "Erlier we said that based on the total derivative we can derive the rule for differentiating our system of equations $f_1(x,y)$ and $f_2(x,y)$ with respect to the variable $t$ when $x$ and $y$ are functions of $t$:\n",
    "$$\n",
    "\\frac{d}{dt} \\begin{bmatrix}\n",
    "f_1(x,y) \\\\\n",
    "f_2(x,y)\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y} \\\\\n",
    "\\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "\\frac{\\partial x}{\\partial t} \\\\\n",
    "\\frac{\\partial y}{\\partial t}\n",
    "\\end{bmatrix}\n",
    "= \\mathbf{J} \\begin{bmatrix}\n",
    "\\frac{\\partial x}{\\partial t} \\\\\n",
    "\\frac{\\partial y}{\\partial t}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Based on this rule we can estimate the end effector velocity using the Jacobian matrix. We can use the following estimation:\n",
    "$$\n",
    "v = \\mathbf{J}(q) * \\mathbf{q}_v = \\mathbf{J}(q) * \\begin{bmatrix}\n",
    "\\frac{\\partial q_1}{\\partial t} \\\\\n",
    "\\frac{\\partial  q_2}{\\partial t} \\\\\n",
    "\\frac{\\partial  q_3}{\\partial t} \\\\\n",
    "\\frac{\\partial  q_4}{\\partial t} \\\\\n",
    "\\end{bmatrix} \n",
    "\\approx J(q) * \\begin{bmatrix}\n",
    "\\Delta q_1 \\\\\n",
    "\\Delta q_2 \\\\\n",
    "\\Delta q_3 \\\\\n",
    "\\Delta q_4 \\\\\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "where $v$ is the end effector velocity, $\\mathbf{J}(q)$ is the Jacobian matrix evaluated at the current joint angles $q$, and $\\mathbf{q}_v$ is the joint velocity vector, which we can estimate with the joint approximate velocities $\\Delta q_1$, $\\Delta q_2$, $\\Delta q_3$, and $\\Delta q_4$.\n",
    "\n",
    "Implement the `endeffector_velocity` function that uses the Jacobian matrix to calculate the end effector velocity. The function should takes in the joint angles `q` and joint velocity vector `q_v`, and returns the end effector velocity `v` calculated using the Jacobian matrix. Use the `openmanipulator_jacobian` function you implemented to calculate the Jacobian matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function endeffector_velocity(q, q_v)\n",
    "    # Calculate the Jacobian matrix at the current joint angles\n",
    "    J = # TODO: calculate the Jacobian matrix of the Openmanipulator \n",
    "\n",
    "    # Calculate the end effector velocity using the Jacobian matrix\n",
    "    v = # TODO: calculate the end effector velocity using the Jacobian matrix and the joint velocities\n",
    "\n",
    "    return v\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function with the following joint angles and joint velocities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_v = [0.0, 0.0, 0.0, 0.0]\n",
    "v = endeffector_velocity(q, q_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the end effector velocity calculation to the Openmanipulator-X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's start the Openmanipulator and subscrib to the joint states\n",
    "- Calculate the end effector velocity using the joint velocities and the Jacobian matrix\n",
    "- Do this by adding a new function which is the new callback function for the joint states subscriber and which calls the `endeffector_velocity` function\n",
    "- print the end effector velocity to the console while you move the Openmanipulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using RigidBodyDynamics\n",
    "using MeshCatMechanisms\n",
    "using MeshCat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcdir = \"../open_manipulator_description/urdf/\"\n",
    "urdf = joinpath(srcdir, \"open_manipulator.urdf\")\n",
    "mechanism = parse_urdf(urdf)\n",
    "mvis = MechanismVisualizer(mechanism, URDFVisuals(urdf));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating of the end effector velocity based on the joint velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = mvis.state\n",
    "joint_state = mvis.state.q\n",
    "set_configuration!(mvis, [0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "zero_velocity!(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function no_control!(torques::AbstractVector, t, state::MechanismState)\n",
    "    qs = state.q[1:4] \n",
    "    q_vs = state.v[1:4]\n",
    "    v = endeffector_velocity(qs, q_vs)\n",
    "    println(\"v: $v\")\n",
    "    total_velocity = norm(v)\n",
    "    println(\"total_velocity: $total_velocity\")\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_time = 2.0\n",
    "ts, qs, vs = simulate(mvis.state, final_time, no_control!; Δt=1e-2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controling the Openmanipulator-X and calculating the end effector velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = mvis.state\n",
    "set_configuration!(mvis, [0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "zero_velocity!(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function joint_position_control(q_state, v_state, q_desired)\n",
    "    kp = 10.0\n",
    "    v_damping = 0.1\n",
    "    max_torque = 15.0\n",
    "\n",
    "    torque = kp * (q_desired - q_state) - v_damping * v_state\n",
    "    torque = clamp.(torque, -max_torque, max_torque)\n",
    "\n",
    "    return torque\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "om_joints = joints(state.mechanism)\n",
    "\n",
    "function simple_joint_control!(torques::AbstractVector, t, state::MechanismState)\n",
    "    target_joint_number = 1\n",
    "    positions = [0.5, 0.0, 0.0, 0.0]\n",
    "\n",
    "    for joint in om_joints\n",
    "        torques[velocity_range(state, joint)] .= 0.0\n",
    "    end\n",
    "    for joint_number in 1:length(om_joints)\n",
    "        if joint_number <= 4\n",
    "            torques[velocity_range(state, om_joints[joint_number])] .= joint_position_control(state.q[joint_number], state.v[joint_number], positions[joint_number])\n",
    "        else\n",
    "            state.v[joint_number] = 0.0\n",
    "        end\n",
    "    end\n",
    "\n",
    "    qs = state.q[1:4]\n",
    "    q_vs = state.v[1:4]\n",
    "    v = endeffector_velocity(qs, q_vs)\n",
    "    total_velocity = norm(v)\n",
    "    println(\"total_velocity: $total_velocity\")\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_time = 5.0\n",
    "ts, qs, vs = simulate(mvis.state, final_time, simple_joint_control!; Δt=1e-2);\n",
    "animate(mvis, ts, qs);\n",
    "joint_state = mvis.state.q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.0",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
